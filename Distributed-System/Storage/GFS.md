怎么存储一个巨大的文件呢？一个很自然的想法：sharding分片，但是大数定理，机器多了必然出现错误，怎么容错fault tolerance呢？紧接着一个想法就是复制replication，有了副本，就要面对一致性consistency问题。为了解决一致性问题，机器之间要有额外的网络交互，即一致性的代价是低性能，这需要一种平衡。
>一致性还有一个含义，就是对用户来说，多个机器看起来就像一个机器。  
>但是这样的一致性有个问题：我们发现，对用户来说多个机器就像一个机器，反过来，一个机器要面对多个用户的读写，那么如果多个用户对同一个内存有冲突的操作，怎么办?
>>不能在用户层面统一语义嘛？

+ 我们来想这个问题，为了多副本，写要多个机器，但是读要在一个机器，不然就没办法容错了，但是多个客户端的矛盾操作，分别到达不同机器的时间不一样，每个机器对操作的处理序列很可能不一样！矛盾后怎么一致性呢？


+ 我们希望GFS有的特性：
	+ 全局通用
	+ sharding
	+ aufomatic recovery

+ 实际实现的局限性
	+ GFS在一个数据中心中（不然应该分布的远一些）
	+ 内部使用
	+ 目标在于极大数据，所以更适合于顺序处理（而不是随机），关注点在巨大吞吐量

+ GFS在SOSP上发表的原因是它的规模非常大，是很工业级别的，而且是弱一致性，很工程而不是科学



## frame
### Master
是Active-Standby模式的

存储文件名和存储位置的对应关系


大量的chunk服务器（每个服务器1到2个desk）


master管理文件和chunk
chunk存储实际数据



master中的一个
一个表：文件名到chunk ID——————————non-volatile, 非易失，保存到硬盘
另一个表：文件名chunk id到chuck data strcur

这个数据
+ 每个chunk存储在那个服务器，一个列表————————这个不保存到硬盘，如果重启则重新通信V（volatile
+ 每个chunk版本号——取决于实现
+ 主chunk，Primary Chunk，因为写要在primary chunk上进行
+ 而每个主chunk只在租约时间内担任主

这些信息保存在内存，定期到硬盘——追加log，生成CheckPoint————————————快照


主chunk不保存，这玩意本来就动态的